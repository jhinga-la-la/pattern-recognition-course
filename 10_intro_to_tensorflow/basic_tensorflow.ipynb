{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (4 points): Basic TensorFlow Computations\n",
    "\n",
    "TensorFlow is a software library for numerical operations which in its core uses the concept of data flow graphs. In such a graph, we can define mathematical operations and let tensor objects flow through the graph to compute an output. The graph representation is especially useful to determine which parts of the code can be executed in parallel (including devices like a GPU). In TensorFlow, this is done automatically without active intervention by the user.\n",
    "\n",
    "In this first exercise, we want to get a basic idea of how such a data flow graph works, which objects we can use and how we define the operations. For this, we use the simple example function\n",
    "\\begin{equation} f(x) = a(x=1)^2 + bx \\end{equation}<br/>\n",
    "with the constants ğ‘ = 2 and ğ‘ = 4. For the test script, please pay also attention to Table 1.\n",
    "\n",
    "1. [Python] Our first goal is to minimize Equation 1 numerically. For this, we need to define a graph which computes ğ‘“ (ğ‘¥) and treats ğ‘¥ as a variable so that we can apply gradient descent to optimize it.\n",
    "\n",
    "    a) Letâ€™s begin with the construction phase.\n",
    "    \n",
    "        i. Define a graph for ğ‘“ (ğ‘¥). ğ‘ and ğ‘ are constants (tf.constant) and ğ‘¥ should be treated as a variable (tf.Variable) with the initial value ğ‘¥0 = 0. The latter is important for our goals since variables maintain their state (their current value) and can be changed by other operations, i.e. ğ‘¥0 is the starting point for gradient descent.\n",
    "    \n",
    "        ii. An optimizer is defined by an operation which changes the variables according to a target function by computing the gradients. Define this operation which applies one step of gradient descent (tf.train.GradientDescentOptimi zer) to our target function ğ‘“ (ğ‘¥). Use a learning rate of ğœ‚ = 0.05.\n",
    "    \n",
    "    b) It is now time to enter the execution phase. We need an active session to perform any operations in our graph. It is common to create a session enclosed in a with block so that the session is closed automatically when leaving the block. The session object also determines the lifetime of the variables as their state is only maintained during the currently active session.\n",
    "    \n",
    "        i. Even though we specified a value for our variable ğ‘¥, we still need to explicitly initialize it. Since it would be a bit laborious to initialize all variables manually, there is the global initializer operation tf.global_variables_initializer() which automatically initializes all variables in the graph. Execute this operation (tf.Operation objects have a run() method which you can use). \n",
    "\n",
    "        ii. Now, apply 30 steps of gradient descent by first executing the training operation and then retrieving the current values for ğ‘“ (ğ‘¥âˆ—) and ğ‘¥âˆ—. Show these values in each epoch. To stay efficient, you should calculate both values in the same graph run, i.e. do not use the eval() method of your tensors. If implemented correctly, the values in the last iteration should be close to the analytic solution ğ‘“ (âˆ’2) = âˆ’6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Python] Letâ€™s plot our function ğ‘“ (ğ‘¥) together with the minimum found. For this, we need\n",
    "to evaluate ğ‘“ (ğ‘¥) over a predefined range. ğ‘¥ has now a different purpose than before: we\n",
    "want to plug in arbitrary values instead of using it as a variable subject to optimization.\n",
    "a) Implement Equation 1 again in a new graph (reset the default one via tf.reset_d\n",
    "efault_graph()) but this time defining ğ‘¥ as a placeholder instead of a variable\n",
    "(tf.placeholder). A placeholder is a node representing an arbitrary tensor which\n",
    "is defined later in the execution phase with concrete values. In the construction\n",
    "phase, they only have a type and a defined shape. If one of the shape components\n",
    "is not known in advance (like here), it is possible to use None instead (the shape is\n",
    "then set during the execution phase).\n",
    "b) In a new session, evaluate ğ‘“ (ğ‘¥) for 50 values in the range [âˆ’6; 2]. You need the\n",
    "feed_dict argument to specify the values for the placeholder ğ‘¥.\n",
    "c) Plot ğ‘“ (ğ‘¥) in the defined range and highlight your minimum ğ‘“ (ğ‘¥âˆ—) found previously\n",
    "in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Pen and Paper] We found the minimum of our function ğ‘“ (ğ‘¥) without ever specifying any gradient. This is possible because TensorFlow implements reverse-mode autodiff and we now want to go through the steps by calculating the derivative ğ‘“ â€²(2) = 16 via Table 2. The basis is the graph of our function as shown in Figure 1.\n",
    "\n",
    "    a) Calculate the forward phase by computing the output of each node ğ‘›ğ‘– for the input ğ‘¥ = 2.\n",
    "\n",
    "    b) Calculate the backward phase by computing âˆ‚ğ‘“/âˆ‚ğ‘›ğ‘–, i.e. the derivative of each node ğ‘›ğ‘– with respect to the target function ğ‘“ (ğ‘¥). For this, you need the chain rule \n",
    "\\begin{equation} \\frac{\\partial f}{\\partial n_i} = \\sum_{n \\in parent(n_i)} \\frac{\\partial f}{\\partial n} \\frac{\\partial n}{\\partial n_i}\\end{equation}\n",
    "where the sum iterates over all parent nodes on which the node ğ‘›ğ‘– depends on (e.g. parent($n_5$) = {$n_6$}). Specify also the parents for the other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Pen and Paper] Which operations in the graph of Figure 1 can be executed in parallel\n",
    "(theoretically)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (6 points): Neural Network in TensorFlow [Python]\n",
    "\n",
    "Even though TensorFlow is a library to built data flow graphs suitable for general purposes, it is mostly used to implement neural networks. In this exercise, we want to see how this works for a simple neural network consisting only of a single hidden layer. There are multiple ways of defining the architecture of a network. Here, we are building it from scratch and in later\n",
    "exercises we are going to use helper functions which hide some of the logic. \n",
    "\n",
    "We are using a small classification problem based on the wine dataset1 as an example. It classifies three different wines based on ğ‘‘ = 13 different (chemical) features for ğ‘› = 178 instances. The dataset has a very good class discrimination so that the neural network should not have any problems with it.\n",
    "\n",
    "Please use the file TensorFlowNeuralNetwork.ipynb attached to this exercise as the basis for your implementation. For the test script, please pay also attention to Table 3.\n",
    "1. The dataset is included in sklearn and you can load it via the load_wine function of the sklearn.datasets package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We need to split the dataset so that we can use one part to train and another to evaluate our network. You can use the function sklearn.model_selection.train_test_s\n",
    "plit for this job (with a test size of 30 %)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The feature dimensions have highly different ranges and it is hence a good idea to standardize our train and test sets (sklearn.preprocessing.StandardScaler). Make\n",
    "sure you scale your test data only with the parameters extracted from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We have now all ingredients together to build our neural network which is also sketched in Figure 2. In the following, use named scopes whenever you think it is suited and add names to your nodes.\n",
    "\n",
    "    a) Create the input layer by defining placeholders for the input data and the labels.\n",
    "        â€¢ The input tensor should have a dimension of ğ‘š Ã— ğ‘‘ whereas ğ‘š denotes the batch size and does not have to be specified yet, i.e. it can be set to None.\n",
    "        â€¢ The tensor storing the class labels is a vector with a length equal to the batch size ğ‘š.\n",
    "\n",
    "    b) Define a hidden layer with 60 neurons which essentially compute the output ğ» = tanh(ğ‘‹ğ‘Š1 + ğ’ƒ1) based on the input ğ‘‹, the weight matrix ğ‘Š1 and the bias vector ğ’ƒ1. We want to learn ğ‘Š1 and ğ’ƒ1 so they have to be defined as variables and initialized properly. ğ’ƒ1 can be set to zero but ğ‘Š1 should be initialized according to a truncated normal distribution with an appropriate standard deviation <br/><br/>\n",
    "\\begin{equation} \\sigma = \\sqrt{\\frac{2}{n_{in} + n_{out}}} \\end{equation}\n",
    "$n_{in}$ and $n_{out}$ denote the number of input and output connections of each neuron, respectively\n",
    "\n",
    "    c) Define the output layer in a similar way but this time using linear transfer functions2 \\begin{equation}y = HW_2 + b_2\\end{equation} Note: the order in which the variables are defined matters for the test script so please define first ğ‘Š1 and ğ’ƒ1 before you define ğ‘Š2 and ğ’ƒ2.\n",
    "    \n",
    "    d) We need an error function to train our network. Since we are dealing with a classification problem, the cross-entropy function is a good choice. You can use $tf.nn.sparse\\_softmax\\_cross\\_entropy\\_with\\_logits$ for this task. The function accepts the labels and the logits directly. Do not apply the softmax function yourself or convert the labels to one-hot vectors.\n",
    "\n",
    "    e) The cross-entropy calculates an error value for each instance. Calculate the average over the complete batch dimension (tf.reduce_mean) to arrive at a scalar value.\n",
    "    \n",
    "    f) Like in Exercise 1, we need a training operation which adjusts the variables in the graph according to our error function. Define this operation but this time using tf.train.AdamOptimizer (default parameters are fine). This is a more advanced optimization technique than classical gradient descent.\n",
    "\n",
    "    g) It is helpful to track the error as well as the accuracy on the training set during the learning process. For this, define a tensor which holds the accuracy\n",
    "\\begin{equation}acc = \\frac{1}{m} \\sum_{i=1}^m correct_i \\hspace{0.5cm} with \\hspace{0.5cm} correct_i = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1, & argmax(y_i) = T_i\\\\\n",
    "            0, & \\quad else\n",
    "        \\end{array}\n",
    "    \\right. \\end{equation}\n",
    "over all ğ‘š instances in the current batch. ğ’šğ‘– denotes the output vector containing the logit value for each class of the ğ‘–-th instance and ğ‘‡ğ‘– âˆˆ {0, 1, 2} denotes the corresponding label.\n",
    "\n",
    "    h) TensorFlow ships with a visualization tool called TensorBoard which can be used to explore the network and its result. To use it, we need to write some data about our model to a log folder which can then later be read by TensorBoard.\n",
    "```python\n",
    "now = datetime.utcnow().strftime('%Y-%m-%d %H;%M;%S')\n",
    "logdir = '{}/run-{}/'.format('tf_logs', now)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "```\n",
    "which creates a log folder and already includes basic information about the graph. The file_writer object can later be used to add more information to the log. Donâ€™t forget to close the file_writer at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We can now enter the execution phase to train and evaluate our network.\n",
    "\n",
    "    a) Run an operation which initializes all variables (similar to Exercise 1).\n",
    "\n",
    "    b) Use 50 epochs to train the network. For simplicity, you can use full batches, i.e. supply all training instances at once (ğ‘› = ğ‘š). After each training step (in a separate graph run), evaluate the model on the training set by calculating the loss and the accuracy.\n",
    "    \n",
    "    c) We can include the evaluation metrics to the log folder so that TensorBoard can plot the result. The error and the loss are both simple scalar values which can be added to the log via\n",
    "```python    \n",
    "loss_summary = tf.Summary(value=[tf.Summary.Value(tag='loss', simple_value=loss_value)])\n",
    "accuracy_summary = tf.Summary(value=[tf.Summary.Value(tag='accuracy', simple_value=acc_value)])\n",
    "file_writer.add_summary(loss_summary, epoch)\n",
    "file_writer.add_summary(accuracy_summary, epoch)\n",
    "```\n",
    "This first creates a binary string (optimized representation used by TensorFlow) and\n",
    "then adds it to the log folder via the file_writer object.\n",
    "\n",
    "    d) After finishing with the training, evaluate the network on the test set. For this, compute the accuracy as well as the confusion matrix. For the latter, you need to retrieve the corresponding logits and then create a vector storing the predicted labels which can be compared with the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Explore your network in TensorBoard. You can open it by executing\n",
    "```python\n",
    "tensorboard --logdir tf_logs\n",
    "```\n",
    "on the command line in the working directory where the Jupyter notebook is located."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
