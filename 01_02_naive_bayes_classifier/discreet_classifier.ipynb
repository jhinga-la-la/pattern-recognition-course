{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Discrete Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to get a basic idea of the naive Bayes classifier by analysing a small example. Suppose we want to classify fruits based on the criteria length, sweetness and the colour of the fruit and we already spent days by categorizing 1900 fruits. The results are summarized in the following table. ![](./img/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep our classifier simple, we deal only with three fruits (banana, papaya and apples). The general scenario is as follows: we get a new fruit where we do not know its class. However, we can measure its length, take a taste and name its colour. Based on these features (the criteria) we then want to know which type of fruit we have. The features are assumed to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding the features and their level of measurement: what scale do they have (i.e. nominal-, ordinal-, interval- or ratio-scaling)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Length: Ordinal <br />\n",
    "Sweetness: Nominal <br />\n",
    "Color: Nominal**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the prior probability of a banana, i.e. 𝑃(𝐵𝑎𝑛𝑎𝑛𝑎)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the prior probability of a long fruit, i.e. 𝑃(𝐿𝑜𝑛𝑔)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the probability that you have a banana in your hand, when you already know\n",
    "that the fruit is long, i.e. 𝑃(𝐵𝑎𝑛𝑎𝑛𝑎|𝐿𝑜𝑛𝑔)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the probability that the fruit is long, when you already know that it is a banana\n",
    "(the likelihood of the evidence), i.e. 𝑃(𝐿𝑜𝑛𝑔|𝐵𝑎𝑛𝑎𝑛𝑎)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The naive Bayes classifier iterates over every possible class and calculates the conditional probability that the class fits the known features. The class with the maximum corresponding probability is selected\n",
    "    $$ argmax_{\\omega \\in \\{𝐵𝑎𝑛𝑎𝑛𝑎, 𝑃𝑎𝑝𝑎𝑦𝑎, 𝐴𝑝𝑝𝑙𝑒\\}} (𝑃(\\omega|x)) $$\n",
    "    <br />\n",
    "    with\n",
    "    <br />\n",
    "    $$ 𝑃(\\omega|x) = \\frac{𝑃(x_1|\\omega) ⋯ 𝑃(x_n|\\omega) ⋅ 𝑃(\\omega)}{𝑝(x)}$$\n",
    "    <br />\n",
    "    For a fruit which has medium length, tastes sweet and looks green to you (i.e. 𝑥 = (𝑀𝑒𝑑𝑖𝑢𝑚, 𝑆𝑤𝑒𝑒𝑡, 𝐺𝑟𝑒𝑒𝑛)<sup>𝑇</sup>), which class does it most likely belong to? Hint: you do not need to calculate 𝑝(𝑥)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Misclassification Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Bayes decision rule, we distinguish between classes solely based on probability distributions. This rule is fixed in the sense that we have no manual control over the decision process. But not every class is of the same importance and sometimes we wish to set an individual focus. This can be achieved with penalty terms and is the topic of this exercise. <br /> <br />\n",
    "We consider a two-class problem and use the normal distributions from the example in the lecture script (slide 27) as likelihood functions\n",
    "\n",
    "$$P(x|\\omega_1) = \\frac{1}{\\sqrt\\pi}\\exp{(-x^2)}$$\n",
    "$$P(x|\\omega_2) = \\frac{1}{\\sqrt\\pi}\\exp{(-(x-1)^2)}$$\n",
    "\n",
    "We do not set concrete values for the a-priori probabilities, yet. But as we only consider two\n",
    "classes, we can use the variable 𝑝 ∈ [0; 1] to describe both\n",
    "\n",
    "$$P(\\omega_1) = p \\hspace{1cm} and  \\hspace{1cm} P(\\omega_2) = 1 − p $$\n",
    "\n",
    "\n",
    "Now, we have all ingredients together to define the loss of each class\n",
    "<br />\n",
    "<br />\n",
    "$$l_1(x) = \\lambda_{21}p(x | \\omega_2)𝑃(\\omega_2)$$\n",
    "$$l_2(x) = \\lambda_{12}p(x | \\omega_1)𝑃(\\omega_1)$$\n",
    "\n",
    "where correct classifications are not penalized, i.e. 𝜆<sub>𝑖𝑖</sub> = 0. The term 𝜆<sub>𝑖𝑗</sub> ∈ ℝ<sup>+</sup> weights the\n",
    "probabilities and penalizes if a pattern which belongs to the class 𝜔<sub>𝑖</sub> is misclassified to the class 𝜔<sub>𝑗</sub>.\n",
    "To assign patterns to classes we use a similar decision rule as in the Bayes case. The only difference is that we select the class which yields the lowest loss instead of the highest probability\n",
    "\n",
    "$$\\omega^* = argmin_i (l_i(x))$$\n",
    "<br />\n",
    "In the case of our two-class problem, we assign a new pattern 𝑥 to 𝜔<sub>1</sub> if 𝑙<sub>1</sub>(𝑥) < 𝑙<sub>2</sub>(𝑥) and to 𝜔<sub>2</sub> if 𝑙<sub>2</sub> (𝑥) < 𝑙<sub>1</sub>(𝑥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain why it is reasonable that the loss of the second class 𝑙<sub>2</sub>(𝑥) uses the probabilities of the first class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose we have 𝑙<sub>1</sub>(𝑥) = 2 and 𝑙<sub>2</sub>(𝑥) = 𝜆<sub>12</sub> ⋅ 3. Define the value(s) of 𝜆<sub>12</sub> which correspond to the following cases:\n",
    "\n",
    "    a) The pattern 𝑥 is assigned to the first class 𝜔<sub>1</sub>.\n",
    "    \n",
    "    b) The pattern 𝑥 is assigned to the second class 𝜔<sub>2</sub>.\n",
    "    \n",
    "    c) The decision is ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The advantage of a two-class problem is that we can reduce the decision of Equation 1 to a simple threshold ̂𝑥<sub>0</sub> separating the regions of the two classes. Find an appropriate ansatz and show that the threshold is given by\n",
    "\n",
    "$$ \\hat x (𝜆_{12}, 𝜆_{21}, p) = \\frac{1}{2} (\\ln( \\frac{𝜆_{12}}{𝜆_{21}} ⋅ \\frac{p}{1 - p}) + 1 ) . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a script which plots the likelihood functions for the parameters 𝜆<sub>12</sub> = 12, 𝜆<sub>21</sub> = 1 and 𝑝 = 1/2 and mark the threshold ̂𝑥<sub>0</sub> as a vertical line in the graph. Plot also the losses 𝑙<sub>1</sub> and 𝑙<sub>2</sub> and mark the threshold value ̂𝑥<sub>0</sub> here as well. An example plot is shown in Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Change the parameters to 𝜆<sub>12</sub> = 7/2, 𝜆<sub>21</sub> = 1 and 𝑝 = 1/3 and plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Change the parameters to 𝜆<sub>12</sub> = 7, 𝜆<sub>21</sub> = 2 and 𝑝 = 1/3 and plot the result. Does the threshold change compared to the previous settings? If not, why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Change the parameters to 𝜆<sub>12</sub> = 1 and 𝜆<sub>21</sub> = 1. For which value(s) of 𝑝 will the threshold line in the likelihood functions be\n",
    "\n",
    "    a) left of the intersection 𝑝(𝑥|𝜔<sub>1</sub>) = 𝑝(𝑥|𝜔<sub>2</sub>),\n",
    "\n",
    "    b) at the intersection 𝑝(𝑥|𝜔<sub>1</sub>) = 𝑝(𝑥|𝜔<sub>2</sub>),\n",
    "    \n",
    "    c) right of the intersection 𝑝(𝑥|𝜔<sub>1</sub>) = 𝑝(𝑥|𝜔<sub>2</sub>)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
