{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (3 points): Decision Surfaces via Gaussian Density Functions [Pen and Paper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Bayes classification process, we try to find decision surfaces which separate the classes in the feature domain. These surfaces define a region for each class so that new data points can easily be assigned to one of the classes. In this exercise, we want to analyse different decision surfaces based on multivariate Gaussian densities. \n",
    "<br />\n",
    "<br />\n",
    "We operate in the two-dimensional feature space and use a two-class problem, i.e. a point ğ‘¥ âˆˆ â„<sup>2</sup> is either assigned to the class ğœ”<sub>1</sub> or the class ğœ”<sub>2</sub>. With the Bayes rule, the resulting surface is defined by the corresponding posterior probabilities of the two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(\\omega_1|x) = P(\\omega_2|x) $$\n",
    "<br />\n",
    "$$ \\frac{p(x|\\omega_1) â‹… P(\\omega_1)}{p(x)} = \\frac{p(x|\\omega_2) â‹… P(\\omega_2)}{p(x)} $$\n",
    "<br />\n",
    "$$ p(x|\\omega_1) â‹… P(\\omega_1) = p(x|\\omega_2) â‹… P(\\omega_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and reduces to the likelihoods ğ‘(ğ‘¥|ğœ”<sub>ğ‘–</sub>) and prior-probabilities ğ‘ƒ(ğœ”<sub>ğ‘–</sub>). For the sake of simplicity, we assume equiprobable classes (ğ‘ƒ(ğœ”<sub>1</sub>) = ğ‘ƒ(ğœ”<sub>2</sub>) = 0.5) so that the decision is solely based on the Gaussian density functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(x|\\omega_i) = \\frac{1}{(2\\pi)^{d/2}|\\sum_i|^{1/2}} exp \\left(âˆ’\\frac{1}{2}(x âˆ’ \\mu_i)^T {\\sum_i}^{-1} (x âˆ’ \\mu_i)\\right) $$\n",
    "<br/>\n",
    "with the mean vector ğœ‡<sub>ğ‘–</sub> and the covariance matrix ğ›´<sub>ğ‘–</sub> for each class ğ‘–."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows three different decision surfaces as well as either a corresponding ğ‘¥<sub>1</sub>- or ğ‘¥<sub>2</sub>-slice. Table 1 summarizes the parameters used to create the figures. Note that we have two Gaussian functions in each case (one for each class). Your task is to map each line in Table 1 to the corresponding decision surface and slice. Please explain your choices, e.g. via sketches.\n",
    "<img src=\"./img/figure1.PNG\">\n",
    "<img src=\"./img/figure2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (7 points): Euclidean Distance, Standardization, Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many applications, it is helpful when we deal with homogeneous data. This is especially\n",
    "important when points should be compared in one way or another. Here, we want to measure\n",
    "the similarity between two points by calculating the corresponding distance. We start with the\n",
    "Euclidean distance, move forward to standardized variables and end up with the Mahalanobis\n",
    "distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the example dataset in Table 2 throughout this exercise. It comprises a toy example which consists of ğ‘› = 7 data points ğ‘¥<sub>ğ‘–</sub> âˆˆ â„<sup>2</sup> = (ğ‘¥<sub>ğ‘–</sub>, ğ‘¦<sub>ğ‘–</sub>) relating education to monthly income. The dataset is also visualized in Figure 2.\n",
    "<img src=\"img/figure3.png\">\n",
    "<img src=\"img/figure4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Pen and Paper] In matrix notation, the Euclidean distance is defined as\n",
    "$$d_E(x_i, x_j) = \\sqrt{(x_i âˆ’ x_j)^T(x_i âˆ’ x_j)}$$\n",
    "with the data points ğ‘¥<sub>ğ‘–</sub> being represented by column vectors.\n",
    "\n",
    "    a) When you look at Figure 2, what would you expect is the nearest neighbour of the point (1, 1200)?\n",
    "\n",
    "    b) Figure 3 shows graphs where for each data point the nearest neighbour is depicted using different distance types. The graph in Figure 3 (a) corresponds to the Euclidean distance. As you can see, the nearest point to (1, 1200) is (1.9, 1300). Does this result match with your previous expectation? If not, what could be the problem here and why does Figure 2 trick you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Pen and Paper] One way to overcome the previous issue is by standardizing the variables\n",
    "$$ \\tilde{x}_i = \\frac{x_i - \\mu_x }{\\sigma_x} $$\n",
    "with respect to the mean and the variance\n",
    "$$ \\mu_x = \\frac{1}{n} \\sum_{i=1}^n x_i \\hspace{0.5cm} and \\hspace{0.5cm} \\sigma_x^2 = \\frac{1}{n-1} \\sum_{i=1}^n(x_i - \\mu_x)^2 $$\n",
    "The same procedure is applied to theğ‘¦-dimension.\n",
    "    \n",
    "    a) What are the units of the dimensionsÌƒğ‘¥andÌƒğ‘¦and which term of Equation 2 is responsible for this change? As a reminder, the unit ofğ‘¥is years of education and ğ‘¦ denotes monthly income.\n",
    "    \n",
    "    b) After standardization, we can use the Euclidean distance again to compare standardized pointsÌƒğ‘¥ğ‘–= (Ìƒğ‘¥ğ‘–,Ìƒğ‘¦ğ‘–). The result is visualized in Figure 3 (b). We see that using standardized variables mitigates the previous issue as, for example, the nearest\n",
    "neighbour of the point(1, 1200)is now(1.5, 1000). Explain why this is the case, i.e. why does it help to standardize the variables?\n",
    "\n",
    "    c) Besides the two-step procedure (first standardizing, then calculating the Euclidean distance), it is also possible to use a direct formula which operates on the original pointsğ‘¥ğ‘– and standardizes the variables on the fly. For this, show that the equation\n",
    "$$ d_E(\\tilde{x}_i, \\tilde{x}_j) = \\sqrt{(x_i - x_j)^TS^{-1}(x_i - x_j)} = d_S(x_i, x_j) $$\n",
    "holds. The matrix\n",
    "$$ S = \\begin{pmatrix}\n",
    "\\sigma^2_x& 0\\\\\n",
    "0 & \\sigma_y^2\n",
    "\\end{pmatrix} $$\n",
    "contains the variances on the diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Pen and Paper] Even though we compensated for the effect of different variances in each dimension, there is still room for improvement. The trend line in Figure 2 clearly indicates a linear dependency between the variables. This means that points which are located more on the right (longer years of education) have naturally a higher income.<br /> \n",
    "If we also want to account for this effect, standardization is not enough anymore since it only compensates for the variance inside each dimension but the effect we observe here arises from the variance across two dimensions. Namely, the dimension of years of education correlates positively with the monthly income. To compensate for this correlation, we can use the Mahalanobis distance\n",
    "$$d_M(x_i, x_j) = \\sqrt{(x_i âˆ’ x_j)^T{\\sum_i}^{-1}(x_i âˆ’ x_j)}$$\n",
    "where ğ›´ denotes the covariance matrix\n",
    "$$\n",
    "\\sum = \\begin{pmatrix}\n",
    "\\sigma^2_x& \\sigma_{xy}\\\\\n",
    "\\sigma_{yx} & \\sigma_y^2\n",
    "\\end{pmatrix} \\hspace{0.5cm} and \\hspace{0.5cm} \\sigma_{xy} = \\sigma_{yx} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i âˆ’ \\mu_x)(y_i âˆ’ \\mu_y) $$\n",
    "is the covariance between the two feature dimensions.\n",
    "\n",
    "    a) We first want to know how the distance pattern looks like when using the Mahalanobis distance. For this, sketch some isocurves in Figure 2 around the mean . All Ì„ğ‘¥ points ğ‘ on such a curve have the same distance ğ‘‘ âˆˆ â„ to the mean â€– â€– Ì„ğ‘¥ âˆ’ ğ‘â€– â€– = ğ‘‘\n",
    "    \n",
    "    b) The nearest neighbour graph for this distance type is shown in Figure 3 (c). Explain with the help of isocurves why the nearest neighbour of (5, 3200) is (7, 4200) and not (5.5, 2400) as it was the case in Figure 3 (b). Hint: you are free to move the isocurves in the coordinate system, i.e. they are translation-invariant.\n",
    "    <img src=\"img/figure5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Python] Finally, we want to implement the different distance types and compute some of the distances between points of Table 2.\n",
    "\n",
    "    a) Write a function distance(xi, xj, matrixA) which implements\n",
    "$$d(x_i, x_j) = \\sqrt{(x_i âˆ’ x_j)^TA^{-1}(x_i âˆ’ x_j)}$$\n",
    "i.e. a general version of the above distance equations with the distance type (Euclidean, standardization, Mahalanobis) being defined by the matrix ğ´.\n",
    "\n",
    "    b) Define the matrix for each distance type for the data listed in Table 2.\n",
    "    \n",
    "    c) Calculate the missing distances of Figure 3. Round your results to two decimal places."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
