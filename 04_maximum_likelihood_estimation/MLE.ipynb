{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Maximum Likelihood Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to estimate the parameters of a probability function by using the maximum likelihood method. We can use this method if we know (or assume) the type of a distribution but donâ€™t have a clue about its parameters. Then, the general idea is to take a random sample and choose the parameters with the highest likelihood of producing the data.\n",
    "<br />\n",
    "\n",
    "As an example, suppose you participate in a lottery where each ticket is either a winning or a blank. You are, of course, interested in the event <em>ğ´ = The lottery ticket rewards you with a price</em>. You donâ€™t know anything about the distribution of the lottery tickets, but you are interested in figuring out how long it takes to get a price. For this, you define a random variable <em>ğ‘‹ = Number n of lottery tickets to buy until A happens</em>, i.e. what is the probability that the ğ‘›-th lottery ticket is a winner?\n",
    "<br />\n",
    "\n",
    "This an ideal problem for the geometric distribution where the probability mass function is defined as\n",
    "$$ P(X = n) = p(1 âˆ’ p)^{n-1} $$\n",
    "Note that this is a discrete distribution with only one degree of freedom (the success probability\n",
    "ğ‘ = ğ‘ƒ(ğ´) âˆˆ ]0; 1[) It returns the probability that the first success happens in the ğ‘›-th trial (ğ‘› = 1, 2, â€¦). Therefore, it suits perfectly to the random variable ğ‘‹. \n",
    "\n",
    "Aglow with enthusiasm, you take three runs buying lottery tickets. In the first run, you get a price for the second ticket. In the second run, you are lucky and already get a price for the first ticket. But luck leaves you in the last run where you need to buy five tickets before getting a price. Summarizing, we have \n",
    "$$ n = \\begin{pmatrix}\n",
    "n_1\\\\\n",
    "n_2\\\\\n",
    "n_3\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "2\\\\\n",
    "1\\\\\n",
    "5\\\\\n",
    "\\end{pmatrix} $$\n",
    "with ğ‘›<sub>ğ‘–</sub> denoting the number of tickets required to get a price in the ğ‘–-th run. Assume that the runs are independent of each other and that the ğ‘›<sub>ğ‘–</sub> arise from identically distributed random variables ğ‘‹<sub>ğ‘–</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Pen and Paper] The goal of the likelihood function ğ¿(ğ‘) is to find the most probable parameter ğ‘ which suits the sample vector ğ‘›. For this, we need to consider the probability of the joint occurrence of all runs ğ‘‹<sub>ğ‘–</sub> <br />\n",
    "<br />\n",
    "$$ L(p) = P({X_1 = n_1} âˆ© {X_2 = n_2} âˆ© {X_3 = n_3}) $$\n",
    "<br />\n",
    "Make this formula explicit and simplify as much as possible. Hint: remember that we assumed independent and identically distributed variables.\n",
    "\n",
    "\n",
    "2. [Pen and Paper] Find the estimate of the success parameter $\\hat{p}$ by maximizing ğ¿(ğ‘). You can assume that the estimate $\\hat{p}$ is a maximum and not a minimum or saddle point (this is also verified visually in the next step). Hint: ğ‘ = 0 and ğ‘ = 1 are excluded by definition.\n",
    "\n",
    "\n",
    "3. [Python] Plot ğ¿(ğ‘) in the range [0; 1] and label the estimated parameter $\\hat{p}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Parzen Window (1D) [Pen and Paper]\n",
    "\n",
    "Sometimes, we donâ€™t even know the type of probability density function (PDF) which produced the observed data. In these cases, we can use non-parametric methods like the Parzen window estimator (kernel density estimation). Let ğ‘¥ğ‘– âˆˆ â„ğ‘‘ (ğ‘– = 1, â€¦ , ğ‘) denote our samples from the dataset. Then, the estimate of the PDF is given by\n",
    "\n",
    "$$ \\tilde{p}(ğ‘¥) = \\frac{1}{h^d â‹… N} \\sum_{i=1}^N K \\left( \\frac{x_i - x}{h} \\right) $$\n",
    "\n",
    "with the side length â„, ğ‘ total number of data points from the ğ‘‘-dimensional space and ğ¾\n",
    "as the kernel function. In this exercise, we want to get used to this method by analysing a\n",
    "one-dimensional example.\n",
    "\n",
    "1. Take a look at this animation<sup>1</sup> which uses standard normal Gaussian functions as kernels on a small dataset ğ‘‹ = (ğ‘¥<sub>1</sub>, ğ‘¥<sub>2</sub>, ğ‘¥<sub>3</sub>, ğ‘¥<sub>4</sub>) = (1, 1.5, 3, 5) which consists only of ğ‘ = 4 values. Describe the effect of increasing side length â„ on\n",
    "\n",
    "    a) the kernel functions $\\hat{K}$<sub>ğº</sub>(ğ‘¥) shown in the bottom of the graph,\n",
    "\n",
    "    b) the histogram and\n",
    "    \n",
    "    c) the resulting probability estimation $\\hat{p}$(ğ‘¥)\n",
    "<br/>\n",
    "\n",
    "2. What do you think is a suitable value of â„ for the dataset ğ‘‹?\n",
    "\n",
    "\n",
    "3. Now, suppose that instead of Gaussians uniform distributed kernels centred at the origin are used\n",
    "<br/>\n",
    "$$K_U(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1, & \\quad -0.5 \\leq x \\leq 0.5\\\\\n",
    "            0, & \\quad else\n",
    "        \\end{array}\n",
    "    \\right. $$\n",
    "Draw the probability estimate $\\hat{p}$(ğ‘¥) for the following three cases:\n",
    "\n",
    "    a) â„ = 0.5\n",
    "\n",
    "    b) â„ = 1\n",
    "    \n",
    "    c) â„ = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Parzen Window (2D) [Python]\n",
    "\n",
    "In this part, we want to apply the Parzen window method to a more complicated example. We want to find a probability estimate for the two-dimensional dataset shown in Figure 1. As kernels, we are using multivariate standard normal Gaussian functions parameterized by\n",
    "$$ \\mu = \\begin{pmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\end{pmatrix}\n",
    "\\hspace{0.5cm} and \\hspace{0.5cm} \\sum = \\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "Note: this exercise is best implemented in a Jupyter Notebook to leverage some of the interactive\n",
    "functionalities.\n",
    "\n",
    "1. The data is stored in the attached data.pickle file. You can load it in your Python program via The file contains 200 data points organized in a matrix of size 200 Ã— 2, i.e. each row corresponds to one observation point.\n",
    "\n",
    "2. Define a standard normal kernel with the parameters of Equation 2. You can create a distribution object with the function <em>multivariate_normal</em> from the <em>scipy.stats</em> package. Make sure you know how to retrieve multiple density values at once, e.g. for the points ğ‘¥<sub>1</sub> = (0, 0) and ğ‘¥<sub>2</sub> = (1, 1) at the same time.\n",
    "\n",
    "3. Write a function <em>estimateProbability(x, h)</em> which computes the density $\\hat{p}$(ğ‘¥) for an arbitrary point ğ‘¥ âˆˆ â„<sup>2</sup> based on the side length â„, i.e. Equation 1 with ğ‘‘ = 2.\n",
    "\n",
    "4. Calculate and plot $\\hat{p}$(ğ‘¥) Ì‚in the range (ğ‘¥, ğ‘¦) âˆˆ [âˆ’400, 400] Ã— [âˆ’400, 400] with step sizes of ğ›¥ğ‘¥ = ğ›¥ğ‘¦ = 10. Play around with different values of the side length â„. Some general hints for this part:\n",
    "\n",
    "    â€¢ The function Axes3D.plot_surface from the mpl_toolkits.mplot3d package creates a surface plot which can be used to visualize the resulting PDF.\n",
    "\n",
    "    â€¢ Add the magic command %matplotlib notebook to interact with the 3D plot (e.g. rotate with the left mouse button, zoom with the right mouse button).\n",
    "\n",
    "    â€¢ It is possible to enrich the user experience of Jupyter Notebooks with interactive widgets. In our case, a simple slider (interact function) may be helpful to test different values of the side length â„.\n",
    "\n",
    "\n",
    "5. Based on your results, which side length â„ would you choose as a good estimate for the data?\n",
    "\n",
    "\n",
    "6. You are listening to a discussion between colleagues about the Parzen window estimator and especially about the side length â„. You hear how one of them is arguing â€œIf you have no idea which side length to choose, just use â„ = 1. This will always lead to a good approximation.â€. What do you think about this statement? Consider what you chose for â„ in this and the previous exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
