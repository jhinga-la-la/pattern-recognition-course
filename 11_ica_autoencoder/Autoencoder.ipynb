{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points): Independent Component Analysis [Python]\n",
    "\n",
    "\n",
    "While PCA (and SVD for that matter) tries to seek new axes which are designed to explain the variance in the data and hence remove correlations between feature dimensions (the covariance matrix of PCA transformed data is equal to the identity matrix), one can go even one step further and also remove statistical dependence between the data which is stronger than any correlation\n",
    "removal technique. As a result, we retrieve the independent components which produced the original data. The algorithm designed for this purpose is called independent component analysis (ICA) and in this exercise, we are going to take a look at it.\n",
    "\n",
    "Note: this exercise is based on the introduction to ICA in [1]. In case you have any questions about the theoretical background of the calculations we are going to perform here, feel free to read the corresponding section in this (very well written) paper.\n",
    "\n",
    "In ICA, we assume a linear mixture ùê¥ between the sources in ùëÜ producing the output ùëã. A common example is the so-called cocktail party problem [1] where we have two independent audio sources (e.g. music and voice) and two different recordings of the sources (e.g. two microphones). A linear combination of the two produced signals reaches each microphone so that our recordings ùëã are mixed together $as^1$ <br/><br/>\n",
    "\\begin{equation}X = SA \\end{equation}\n",
    "\n",
    "where both, ùëã and ùëÜ, are matrices with the columns representing variables (here audio sources) and the rows observations (audio samples over time). In our case, we are using two sources which were manually combined over 10k samples. Hence, $X, S \\in \\mathbb{R}^{10000√ó2}$ and $A \\in \\mathbb{R}^{2√ó2}$. You only get the two ‚Äúrecorded‚Äù audio signals in ùëã and your task is to find the original source signals in $S$.\n",
    "\n",
    "Mathematically, we need the inverse of the matrix ùê¥ to recover ùëÜ. A common approach is to\n",
    "simplify this by applying SVD first on ùê¥ so that the problem reduces to find three individual\n",
    "matrices [1]\n",
    "    \\begin{equation}S = XA^{‚àí1} = X(U\\Sigma VT)^‚àí1 = XV\\Sigma^{-1}U T.\\end{equation}\n",
    "\n",
    "In the following, plot the result after each calculation. A summary of all plots which you should have in the end is shown in Figure 1. If you want to play the audio files in your Jupyter notebook, you can use the Audio function from the IPython.display package. For the test script, please pay also attention to Table 1.\n",
    "\n",
    "1. The two audio signals are stored in the files $mixedA.wav$ and $mixedB.wav$. Load both files into your Python program and store them in one matrix (this is ùëã). Note: the WAV files use the 32-bit floating-point format so that the loaded signal values should already be in the range [‚àí1; 1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate the centralized data $X_c$ by subtracting the (column) mean $\\mu_i$\n",
    "\n",
    "\\begin{equation}X_c = X ‚àí (\\mu_1 \\hspace{0.2cm} \\mu_2) \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We approach Equation 1 from the left side. It turns out that ùëâ contains the eigenvectors of $X_c^T X_c$ in the columns and $\\Sigma$ stores the square root of the eigenvalues of $X_c^TX_c$ on the diagonal (even though SVD was applied to ùê¥ and not $X_c$). Hence, we need to apply PCA to $X_c$ first and use the result to transform the data\n",
    "\\begin{equation}X_w = X_c V \\Sigma^{-1} \\end{equation}<br/>\n",
    "This process is called whitening or sphering [1] of the data and there is also an option in sklearn.decomposition.PCA which you can use to calculate $X_w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The next step is to calculate $U^T$. To do so, we exploit the fact that $U^T$ is essentially a rotation matrix. This leads to a new (and not really scalable) approach to find $U^T$: we simply try a few rotation angles ùúÉ and choose the one which fits best.\n",
    "To find out if a rotation matrix leads to a good or bad result, we exploit the fact of assumed independence of the signals and use a measure which tells us how independent our data currently are. This can be achieved by minimizing the so-called multi-information [1] of the rotated data2\n",
    "\\begin{equation}\\theta^* = argmin_{\\theta} \\sum_{i=1}^2H ((X_wR(\\theta))_i)\\end{equation}\n",
    "where H calculates the entropy based on the ùëñ-th column of the data transformed via the rotation matrix $R(\\theta)$.\n",
    "\n",
    "    a) Calculating the entropy of a vector ùíô ‚àà ‚Ñù10000 is not really applicable since it is not unlikely that most of the values occur only once leading lead to a very high entropy value which may not even change for different ùúÉ values. One way to overcome this issue is by grouping similar values together and then calculate the entropy from these groups. More concretely, this can be achieved by calculating a normalized histogram so that we have a discrete probability ùëÉ(ùëñ) for each of the ùêµ bins. Then, we can simply apply the formula of the entropy \n",
    "\\begin{equation}H = \\sum_{i=1}^BP(i).log_2(P(i))\\end{equation}\n",
    "Write a function gentropy(values) which calculates the entropy based on this histogram approach. The number of bins ùêµ is crucial here since it determines how many values are grouped together and are hence considered equal for the entropy calculation. You can set it to ùêµ = 500.\n",
    "\n",
    "    b) Test 180 different angular values from the range [0; ùúã] and calculate the multiinformation with the help of your gentropy function and extract the angular value $\\theta^*$ with the lowest corresponding multi-information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We now have all ingredients together to reconstruct the original signals.\n",
    "\n",
    "    a) Apply the final transformation matrix\n",
    "\\begin{equation}X_r = X_wU^T = X_wR(\\theta^*)\\end{equation}\n",
    "\n",
    "    b) We can‚Äôt reconstruct the original amplitude values with ICA. Hence, the reconstruction is only correct up to a scaling factor. To avoid overdrive, scale the reconstructed signals $X_r$ to the range [‚àí1; 1] via min-max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (5 points): Autoencoder [Python]\n",
    "\n",
    "An autoencoder is an unsupervised learning technique based on neural networks which can be used to learn new features. The general idea is to construct a network hierarchy with an equal number of neurons in the input as well as in the output layer and use the input also as the teaching signal. Further, a bottleneck is introduced in the middle of the network hierarchy so that a simple forwarding of the signal is not possible. Instead, the network is forced to learn representative features to recover the data even when only a subset of the information is available.\n",
    "\n",
    "There are three common use cases of an autoencoder: data denoising, variation learning and dimensionality reduction. In this exercise, we are going to apply the latter to a small example in order to explore the basics of autoencoders. We have already covered some dimensionality reduction techniques and it turns out that there is a relationship between autoencoders and PCA. We take this as a starting point and then extend to the general concept of non-linear PCA (with similar results as kernel PCA), i.e. instead of lines as main axes of our new coordinate system (eigenvectors) we now may end up with more arbitrary shapes which can reflect the data distribution better.\n",
    "\n",
    "This exercise comes with some predefined code in the Autoencoder.ipynb Jupyter notebook. Use this file as the starting point for your implementation. As you can see, we are using a two-dimensional dataset here so that our projections are going to be one-dimensional. For the test script, please pay also attention to Table 2.\n",
    "\n",
    "1. We can construct a very simple autoencoder for our dataset by using two input neurons, one neuron in the coding layer (bottleneck) and two output neurons as well as linear transfer functions. Then, we basically compute PCA and the output of the coding layer corresponds to the projections (also called codings) and the output of the network to the recovered data points based on these projections. \n",
    "\n",
    "    a) This type of autoencoder is already implemented. Familiarize yourself with the code and execute it.\n",
    "    \n",
    "    b) As a comparison, compute PCA also in the normal way and extract the first eigenvector $\\mu_1$.\n",
    "    \n",
    "    c) Visualize the data, the output of the autoencoder, the line of the eigenvector $\\mu_1$ and some contour lines which show how the data are projected onto the new axis. That is, all points on one contour line map to the same projection value. A possible result is shown in Figure 3 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We now want to change the autoencoder so that it performs non-linear PCA instead.\n",
    "\n",
    "    a) Implement the hierarchy as shown in Figure 4 with the following notes in mind:\n",
    "        ‚Ä¢ The en- and decoder now include an additional hidden layer with 20 neurons each.\n",
    "        ‚Ä¢ The hidden layers use tanh(ùë•) as transfer functions and the code as well as the output layer use linear transfer\n",
    "        functions.\n",
    "        ‚Ä¢ It is recommended to constrain the autoencoder network in some way or another to get good codings. Here, we want to regularize all weights by adding an L1 penalty term to the error function\n",
    "\\begin{equation}E = E_0 + \\lambda \\sum_w |w| \\end{equation}\n",
    "        with the base loss $E_0$ (MSE) and the regularization term ùúÜ = 0.002. Note: you need to find out yourself how this can be implemented in TensorFlow.\n",
    "        \n",
    "    b) Visualize the result in the same way as before (without the eigenvector $\\mu_1$). A possible solution is shown in Figure 3 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Do you think that it would be a good idea to use a non-linear transfer function (like tanh(ùë•)) in the output layer (for our dataset)? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Figure 2 shows a histogram of the projected values (the codings) of both autoencoders. As you can see, the histogram of the non-linear autoencoder is more balanced whereas in the linear case we have relatively clear peaks. Explain why we observe these differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
