{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (4 points): Idea of Maximal Margin (SVM) [Pen and Paper]\n",
    "\n",
    "With the help of support vector machines, we can find a separation line which maximize the margin between two classes. In this exercise, we use a two-class example and approach to such a line in a geometric way. Figure 1 shows the two-dimensional data we are using throughout this exercise. Each point 𝑥 ∈ ℝ<sup>2</sup> belongs either to the positive 𝜔<sub>1</sub> (blue, circles) or negative class 𝜔<sub>−1</sub> (orange, squares). \n",
    "\n",
    "1. To find the separation line, the convex hull of a dataset is helpful. Start by drawing the convex hull for each class in Figure 1.\n",
    "\n",
    "2. The line with the maximal margin is defined by the two points from the convex hulls of the two classes with minimal distance to each other. Find these two points on the convex hulls you have just drawn and label them with 𝑐<sub>1</sub> and 𝑐<sub>-1</sub> (for the class 𝜔<sub>1</sub> and 𝜔<sub>-1</sub>, respectively). Note that every point on a convex hull is a possible candidate and they do not necessarily need to correspond with the data points.\n",
    "\n",
    "3. Connect the points 𝑐<sub>1</sub> and 𝑐<sub>-1</sub> with a line.\n",
    "\n",
    "4. The separation line goes through the centre of the line you have just drawn and is orthogonal to it, i.e. the two lines enclose a 90° angle. Draw the separation line 𝑠, the line 𝑙<sub>1</sub> which passes through the support vectors of 𝜔<sub>1</sub> and the line 𝑙<sub>-1</sub> which passes through the support vectors of 𝜔<sub>-1</sub>.\n",
    "\n",
    "5. Add two arbitrary points from each class, i.e. x<sub>1</sub>, 𝑥<sub>2</sub> ∈ $\\omega$𝜔<sub>1</sub> and 𝑥<sub>3</sub>, 𝑥<sub>4</sub> ∈ 𝜔<sub>-1</sub>, to the feature space so that the separation line 𝑠 found previously does not change.\n",
    "\n",
    "6. Start fresh with Figure 2 and add a new data point 𝑥<sub>5</sub> which belongs to a class of your choice to the feature space so that the new margin between the two classes equals 1. As before, sketch the three lines $\\tilde{l}$<sub>1</sub> , $\\tilde{l}$<sub>-1</sub> and $\\tilde{s}$ based on the points $\\tilde{c}$<sub>1</sub> and $\\tilde{l}$<sub>-1</sub> on the convex hull.\n",
    "\n",
    "7. In general, what is the difference between a line obtained via SVM and a line obtained via the perceptron learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (6 points): SVM [Pen and Paper]\n",
    "\n",
    "From the previous exercise, we know which kind of separation line we can expect when using support vector machines and how to find it graphically. Determining the separation line analytically, however, is very hard; even for the simple problem of Exercise 1. The number of variables and conditions make this task impractical for a “Pen and Paper” exercise. But, to still get a basic idea of the SVM algorithm, we consider here an even simpler problem where we have only two points $x_i \\in \\mathbb{R}^{2}$ which belong to their own class each. The first point is from the set of positive samples $\\omega_1 = \\{x_i | T_i = 1\\}$ and the second from the set of negative samples $\\omega_{-1} = \\{x_i | T_i = −1\\}$. The two points are defined as\n",
    "$$ x_1 = \n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix} \\in \\omega_1\n",
    "\\hspace{0.5cm}and\\hspace{0.5cm}\n",
    "\\begin{pmatrix}\n",
    "2\\\\\n",
    "3\\\\\n",
    "\\end{pmatrix} \\in \\omega_{-1}\n",
    "$$\n",
    "\n",
    "Obviously, both points are also support vectors since they are the only representatives from their classes and hence must lie on the margin. Our goal is to find the parameters of a separation line with maximal margin, i.e. we search for 𝑤 and 𝑤<sub>0</sub>. In order to find our parameters, we need to optimize the Lagrange function\n",
    "\\begin{equation}\n",
    "L(w, w_0, \\alpha_1, \\alpha_2) = L(\\alpha_1, \\alpha_2) = \\sum_{i=1}^2 \\alpha_i - \\frac{1}{2}\\sum_{i=1}^2\\sum_{j=1}^2\\alpha_i\\alpha_jT_iT_jx_i^Tx_j\n",
    "\\end{equation}\n",
    "subject to the constraint\n",
    "$$\\sum_{i=1}^2\\alpha_iT_i=0$$\n",
    "\n",
    "Technically, we also need the constraints $\\alpha_i \\geq 0$. But to keep things simple, we are assuming them satisfied here. \n",
    "1. Set up the Lagrange function (Equation 1) as well as the constraint (Equation 2) of our problem set, i.e. plug in the values and simplify.\n",
    "\n",
    "2. To optimize Equation 1 under the constraint of Equation 2, we need an additional Lagrange function with the Lagrange multiplier 𝜆\n",
    "\\begin{equation} \\Lambda(\\alpha_1, \\alpha_2, \\lambda) = L(\\alpha_1, \\alpha_2) + \\lambda\\left(\\sum_{i=1}^2\\alpha_iT_i\\right) \\end{equation}\n",
    "Optimize this function and show that the optimal values are given as\n",
    "$$\\alpha_1^* = \\frac{2}{5} \\hspace{0.5cm} and \\hspace{0.5cm} \\alpha_2^* = \\frac{2}{5} $$\n",
    "It is sufficient to calculate only the extrema values. You do not need to show that they belong to a minimum/maximum (and not a saddle point) of $L(\\alpha_1, \\alpha_2)$. You can use Figure 3 to convince yourself that this is indeed the case here.<br/>\n",
    "3. Based on the previous results, calculate the line parameters 𝑤 and 𝑤<sub>0</sub>.<br/>\n",
    "4. Draw the resulting separation line into Figure 4.<br/>\n",
    "5. Suppose you want to classify the new point 𝑥3 = (2, 2). Assign this point the correct label\n",
    "    \n",
    "    a) graphically based on the line you have just drawn into Figure 4.\n",
    "    \n",
    "    b) analytically by using the decision function 𝑓(𝑥) (script page 102).<br/>\n",
    "<br/>    \n",
    "6. Some general questions to conclude this exercise (no calculations required):\n",
    "    \n",
    "    a) How would Equation 1 change if we had more than three points?\n",
    "    \n",
    "    b) How can we infer from the coefficient 𝛼∗ 𝑖 whether the corresponding point 𝑥𝑖 contributes to the separation line (i.e. whether 𝑥𝑖 is a support vector)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
