{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Discrete Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to get a basic idea of the naive Bayes classifier by analysing a small example. Suppose we want to classify fruits based on the criteria length, sweetness and the colour of the fruit and we already spent days by categorizing 1900 fruits. The results are summarized in the following table. ![](./img/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep our classifier simple, we deal only with three fruits (banana, papaya and apples). The general scenario is as follows: we get a new fruit where we do not know its class. However, we can measure its length, take a taste and name its colour. Based on these features (the criteria) we then want to know which type of fruit we have. The features are assumed to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding the features and their level of measurement: what scale do they have (i.e. nominal-, ordinal-, interval- or ratio-scaling)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Length: Ordinal <br />\n",
    "Sweetness: Nominal <br />\n",
    "Color: Nominal**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the prior probability of a banana, i.e. ğ‘ƒ(ğµğ‘ğ‘›ğ‘ğ‘›ğ‘)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the prior probability of a long fruit, i.e. ğ‘ƒ(ğ¿ğ‘œğ‘›ğ‘”)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the probability that you have a banana in your hand, when you already know\n",
    "that the fruit is long, i.e. ğ‘ƒ(ğµğ‘ğ‘›ğ‘ğ‘›ğ‘|ğ¿ğ‘œğ‘›ğ‘”)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the probability that the fruit is long, when you already know that it is a banana\n",
    "(the likelihood of the evidence), i.e. ğ‘ƒ(ğ¿ğ‘œğ‘›ğ‘”|ğµğ‘ğ‘›ğ‘ğ‘›ğ‘)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The naive Bayes classifier iterates over every possible class and calculates the conditional probability that the class fits the known features. The class with the maximum corresponding probability is selected\n",
    "    \\begin{equation} argmax_{ğœ” âˆˆ \\{ğµğ‘ğ‘›ğ‘ğ‘›ğ‘, ğ‘ƒğ‘ğ‘ğ‘ğ‘¦ğ‘, ğ´ğ‘ğ‘ğ‘™ğ‘’\\}} (ğ‘ƒ(ğœ”|ğ‘¥)) \\end{equation}\n",
    "    <br />\n",
    "    with\n",
    "    <br />\n",
    "    \\begin{equation}ğ‘ƒ(ğœ”|ğ‘¥) = \\frac{ğ‘ƒ(ğ‘¥1|ğœ”) â‹¯ ğ‘ƒ(ğ‘¥ğ‘›|ğœ”) â‹… ğ‘ƒ(ğœ”)}{ğ‘(ğ‘¥)}\\end{equation}\n",
    "    <br />\n",
    "    For a fruit which has medium length, tastes sweet and looks green to you (i.e. ğ‘¥ = (ğ‘€ğ‘’ğ‘‘ğ‘–ğ‘¢ğ‘š, ğ‘†ğ‘¤ğ‘’ğ‘’ğ‘¡, ğºğ‘Ÿğ‘’ğ‘’ğ‘›)<sup>ğ‘‡</sup>), which class does it most likely belong to? Hint: you do not need to calculate ğ‘(ğ‘¥)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Misclassification Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Bayes decision rule, we distinguish between classes solely based on probability distributions. This rule is fixed in the sense that we have no manual control over the decision process. But not every class is of the same importance and sometimes we wish to set an individual focus. This can be achieved with penalty terms and is the topic of this exercise. <br /> <br />\n",
    "We consider a two-class problem and use the normal distributions from the example in the lecture script (slide 27) as likelihood functions\n",
    "\n",
    "\\begin{equation}P(ğ‘¥|ğœ”_1) = \\frac{1}{\\sqrt\\pi}\\exp{(-x^2)}\\end{equation}\n",
    "\\begin{equation}P(ğ‘¥|ğœ”_2) = \\frac{1}{\\sqrt\\pi}\\exp{(-(x-1)^2)}\\end{equation}\n",
    "\n",
    "We do not set concrete values for the a-priori probabilities, yet. But as we only consider two\n",
    "classes, we can use the variable ğ‘ âˆˆ [0; 1] to describe both\n",
    "\n",
    "\\begin{equation}P(ğœ”_1) = p \\hspace{1cm} and  \\hspace{1cm} P(ğœ”_2) = 1 âˆ’ p \\end{equation}\n",
    "\n",
    "\n",
    "Now, we have all ingredients together to define the loss of each class\n",
    "<br />\n",
    "<br />\n",
    "\\begin{equation}l_1(ğ‘¥) = ğœ†_{21}p(x | ğœ”_2)ğ‘ƒ(ğœ”_2)\\end{equation}\n",
    "\\begin{equation}l_2(ğ‘¥) = ğœ†_{12}p(x | ğœ”_1)ğ‘ƒ(ğœ”_1)\\end{equation}\n",
    "\n",
    "where correct classifications are not penalized, i.e. ğœ†<sub>ğ‘–ğ‘–</sub> = 0. The term ğœ†<sub>ğ‘–ğ‘—</sub> âˆˆ â„<sup>+</sup> weights the\n",
    "probabilities and penalizes if a pattern which belongs to the class ğœ”<sub>ğ‘–</sub> is misclassified to the class ğœ”<sub>ğ‘—</sub>.\n",
    "To assign patterns to classes we use a similar decision rule as in the Bayes case. The only difference is that we select the class which yields the lowest loss instead of the highest probability\n",
    "\n",
    "\\begin{equation}ğœ”^* = argmin_ğ‘– (ğ‘™_ğ‘–(ğ‘¥)) \\end{equation}\n",
    "<br />\n",
    "In the case of our two-class problem, we assign a new pattern ğ‘¥ to ğœ”<sub>1</sub> if ğ‘™<sub>1</sub>(ğ‘¥) < ğ‘™<sub>2</sub>(ğ‘¥) and to ğœ”<sub>2</sub> if ğ‘™<sub>2</sub> (ğ‘¥) < ğ‘™<sub>1</sub>(ğ‘¥)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
