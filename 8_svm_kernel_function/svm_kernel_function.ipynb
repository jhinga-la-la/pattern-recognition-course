{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (7 points): Kernel Functions\n",
    "\n",
    "We already learned techniques to classify linearly separable data, e.g. with the help of an SVM. Unfortunately, not every dataset is linearly separable. There are cases where we need more complex decision surfaces. Ideally, we would still like to use the same algorithms as in the linear case. A common approach is to use so-called kernel functions which project the data\n",
    "into another space so that the representations of the points in the new space become linearly separable. For example, we can project our data to a higher dimensional space.<br/>\n",
    "\n",
    "In this exercise, we want to take a look at how this works in the case of SVMs. For this, we are going to work with a one-dimensional example consisting of four values $x_i \\in \\mathbb{R}$\n",
    "\\begin{equation} x_1 = ‚àí2, x_4 = 2 \\in \\omega_1 \\hspace{0.5cm} and \\hspace{0.5cm} x_2 = ‚àí1, x_3 = 0 \\in \\omega{‚àí1} \\end{equation}\n",
    "\n",
    "assigned to either the set of positive samples $\\omega_1 = {x_i | T_i = 1}$ or the set of negative samples $\\omega{‚àí1} = {x_i | T_i = ‚àí1}$. Further, we take the opportunity and analyse the decision function in a bit more detail.\n",
    "\n",
    "1. [Python] Figure 1 shows a graphical representation of the dataset we are using here. Obviously, the data are not linearly separable. Find a projection function $\\phi ‚à∂ \\mathbb{R} \\rightarrow \\mathbb{R}^2$, $\\tilde{x} = \\phi(x)$ which projects the data into the two-dimensional space so that the projected points $\\tilde{x}$ become linearly separable. Apply $\\phi(x)$ to all the values in the dataset, i.e. calculate $\\tilde{x}_i = \\phi(x_i)$ for ùëñ = 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Python] Once we have calculated all projections $\\tilde{x}_i$, we can apply the same algorithm as in one of the previous exercises. Remember that we need the optimal Lagrange multipliers $\\alpha_i^*$ and the threshold $w_0^*$ to classify new points. Train a linear SVM with the help of the svm object from the sklearn package and extract the relevant parameters, i.e. <br/><br/>\n",
    "\\begin{equation}\\alpha_1^‚àó = ?,\\hspace{0.2cm} \\alpha_2^‚àó = ?,\\hspace{0.2cm} \\alpha_3^‚àó = ?,\\hspace{0.2cm} \\alpha_4^‚àó = ? \\hspace{0.5cm} and \\hspace{0.5cm} w_0^‚àó = ? \\end{equation} <br/>Note that you may have to take the absolute value of the extracted parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Python] Use the calculated parameters to classify the new point ùë•5 = 1 based on the decision function\n",
    "\\begin{equation} f(\\tilde{x}) = sig \\left(\\sum_{i=1}^4 \\alpha_i^‚àóT_i ‚ü®\\tilde{x}_i, \\tilde{x}‚ü© + w_0^‚àó\\right)  \\end{equation}\n",
    "You can check your result by additionally using the predict method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. By now, we calculated the projection of each ùë• value and applied the same algorithm as usual. However, it turns out that projecting each data point is quite computational expensive when the number of points increases. Luckily, there is a so-called kernel trick which avoids the necessity of calculating any projections at all. The idea is to use a kernel function ùëò(ùë•ùëñ, ùë•ùëó) which can be used as a replacement for the dot product of the projected data points <br/><br/>\n",
    "\\begin{equation}(\\tilde{x}_i, \\tilde{x}‚ü© = k(x_i, x_j)\\end{equation}<br/><br/>\n",
    "Note that the kernel function uses the original data points $x$ and not the projections $\\tilde{x}$.\n",
    "\n",
    "    a) [Pen and Paper] Define the kernel $k(x_i, x_j)$ which calculates the dot product of your projection function $\\phi(x)$.\n",
    "\n",
    "    b) [Python] Compute the output of the value $x_5 = 1$ again but this time using only the one-dimensional values $x_i$. For this, you need to change Equation 1 so that it uses your kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [Python] We can apply the kernel trick not only to the decision function but also to the optimization problem itself. Remember that for this we need to optimize <br/><br>\n",
    "\\begin{equation}\n",
    "L(w, w_0, \\alpha_1, \\alpha_2) = L(\\alpha_1, \\alpha_2) = \\sum_{i=1}^2 \\alpha_i - \\frac{1}{2}\\sum_{i=1}^2\\sum_{j=1}^2\\alpha_i\\alpha_jT_iT_jx_i^Tx_j\n",
    "\\end{equation}<br/>\n",
    "subject to the constraint\n",
    "$$\\sum_{i=1}^2\\alpha_iT_i=0$$ <br/><br/>\n",
    "Note how the dot product $\\langle\\tilde{x}_i, \\tilde{x}_j\\rangle$ appears here as well. Re-calculate the optimal parameters by using only the original values $x_i$. For this, write a custom kernel function according to the official interface which implements your previously defined kernel $k(x_i, x_j)$. When creating the svm object, you have the choice to set your own kernel. Check if the resulting parameters are indeed the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. So far, we have only used the trained SVM to predict a label for a new data point based on the decision function of Equation 1. However, it is often useful to retrieve a prediction score as well, i.e. a value which tells us how good the prediction was. That is, we also want information about the uncertainty of the classifier. <br/><br/> In the case of SVMs, we can use the signed distance to the hyperplane as an indicator. As closer a point is located to this hyperplane, as more uncertain is the classification since then the point is also in nearer proximity to the other class. On the other hand, a point far away from the hyperplane has a very clear class assignment. Remember that we can calculate the distance of a point ùíë to a hyperplane ùêª via <br/><br/>\\begin{equation}d(p, H) = \\frac{\\langle w, p \\rangle + w_0}{ ‚Äñw‚Äñ_2 }\\end{equation} <br/><br/> using the weight vector ùíò and the bias $w_0$.\n",
    "\n",
    "    a) [Pen and Paper] It turns out that we already dealt with distances before even though it was not obvious. Show that the decision function of Equation 1 without the sig(ùë•) function already calculates the unnormalized version of Equation 3, i.e. without dividing by $‚Äñùíò‚Äñ_2$\n",
    "\\begin{equation} d(\\tilde{x}, H) ‚ãÖ ‚Äñùíò‚Äñ_2 = \\sum_{i=1}^4\\alpha_i^*T_ik(x_i, x) + w_0^*\\end{equation}\n",
    "Hint: use the definition of ùíò from the script (p. 101). Note also that on the left side the projection ÃÉùíô and on the right side only the value ùë• is used. That is, we can calculate the (unnormalized) distance from the projected point to the hyperplane in\n",
    "the projection space without actually entering this space. \n",
    "\n",
    "    b) [Pen and Paper] In case we want the correct distance to the hyperplane, we need to divide Equation 4 by the norm ‚Äñùíò‚Äñ2. However, this is not an easy task if we use a kernel function where we cannot directly compute the projections (cf. Exercise 2). Show that we can use the alternative expression<br/><br/>\n",
    "\\begin{equation}‚Äñùíò‚Äñ2 = \\sqrt{\\sum_{i=1}^4 \\sum{j=1}^4T_iT_j\\alpha_i^‚àó\\alpha_j^‚àók(x_i, x_j)}\\end{equation} <br/><br/>\n",
    "instead which does not use any variable from the projection space.\n",
    "\n",
    "    c) [Python] Calculate the signed distance of the point $\\phi(x_5)$ to the hyperplane of your trained SVM without using $\\phi(x_5)$ directly. Instead, use only the value $x_5 = 1$ and your kernel $k(x_i, x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (3 points): RBF Kernel [Pen and Paper]\n",
    "\n",
    "Kernel functions are a very neat way to handle non-linearly separable problems. They can be used in all algorithms which need only the dot product of the data points so that the kernel function can be used instead (Equation 2). The power of the kernels is that they operate implicitly in a higher-dimensional projection space without actually entering it. This gives us the\n",
    "opportunity to make our data linearly separable without negative consequences in terms of computational efficiency.\n",
    "\n",
    "There are also some standard kernels often used and one particularly famous example is a radial basis function\n",
    "\\begin{equation}k(x_i, x_j) = e^{‚àí\\gamma‚Äñx_i‚àíx_j‚Äñ_2^2} \\end{equation}\n",
    "controlled by the parameter $\\gamma \\in \\mathbb{R}^+$. This kernel is a bit special. First, it does not contain any dot product so it may be a bit surprising that we can use it to replace the calculation of a dot product. Secondly, it is even more unclear what the projection space for this kernel is, i.e. how the corresponding projection function $\\phi(x)$ is defined. Thirdly, it introduces a new hyperparameter $\\gamma$ which is worth further investigations.\n",
    "\n",
    "This exercise tries to bring some light into this darkness. While we are trying to achieve this,\n",
    "we also take the opportunity and briefly discuss non-separable problems where the margin is\n",
    "calculated by minimizing\n",
    "\\begin{equation} \\varphi(x, \\delta) = \\frac{‚Äñùíò‚Äñ_2^2}{2} + C \\sum_{i=1}^N \\delta_i \\end{equation}\n",
    "with the slack variables $ùõø_i ‚â• 0$ and the new hyperparameter ùê∂ ‚àà ‚Ñù+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It turns out that Equation 5 uses an infinitely dimensional projection space. This is a bit complicated to see so we consider only the one-dimensional case where we reduce our data points to be values ùë• ‚àà ‚Ñù. For this case, show that the projection function is defined as <br/><br/>\n",
    "\\begin{equation}\\phi(x) = e^{‚àí\\gamma x^2} \\left(1, \\sqrt{\\frac{2\\gamma}{1!} ùë•}, \\sqrt{\\frac{(2\\gamma)^2}{2!} x_2}, \\sqrt{\\frac{(2\\gamma)^3}{3!} x_3}, ‚Ä¶\\right) \\end{equation} <br/><br/>\n",
    "Hint: use the Taylor series of the exponential function\n",
    "\\begin{equation}e^x = 1 + ùë• + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + ... = \\sum_{k=0}^\\inf \\frac{x^k}{k!} \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We now want to turn our attention to the hyperparameters ùõæ and ùê∂. For this, open this animation1 where you can adjust both parameters as well as the kernel type. It\n",
    "shows a two-dimensional example of a non-linear separable dataset and plots the decision function of Equation 4 as a heat map.\n",
    "\n",
    "    a) For the parameter ùê∂, fill out Table 1 by answering the following questions. \n",
    "        i. How does the width of the margin change (linear kernel)? [small/large]\n",
    "        ii. How does the complexity of the decision surface change? [simple/complex]\n",
    "        iii. How does the accuracy on the (training) data set change? [low/high]\n",
    "\n",
    "    b) For the parameter ùõæ, fill out Table 2 by answering the following \n",
    "        i. How does the corresponding RBF function change? For this, map each function shown in Figure 2 to the corresponding parameter description. [A/B]\n",
    "        ii. Each data point has a certain influence on the decision function. How strong is this influence? [small/high]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
