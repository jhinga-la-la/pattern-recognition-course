{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Continuous Naive Bayes Classifier\n",
    "\n",
    "In this part of the exercise, we want to apply the naive Bayes classifier to a continuous feature. We are building a small example application which classifies regions in audio signals as either silence or voice. That is, we want to map each position in the audio signal to one of the two classes. As a feature, we use the energy of the audio signal which is defined as the quadratic sum over a window of size 𝑤 > 0\n",
    "\n",
    "$$ E(t) =\\sum\\limits_{i=t}^{\\min(t+w, N)}s(i)^2 $$\n",
    "\n",
    "with 𝑡 as the current position in the signal and 𝑁 as the number of samples. This is a onedimensional feature which we can calculate at every position in the signal. A possible solution for this exercise is shown in Figure 1.\n",
    "<img src=\"img/figure1.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the level of measurement (scale) of the energy feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start by recording two short sentences. Don’t make your audio recordings too long, a few seconds are enough. One file will be used to train and the other to test the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, we can start and import the audio files.\n",
    "\n",
    "    a) To load the files in your Python program, you can use the scipy.io.wavfile.read() function. Store also the sample rates as we need them later. Note: when you recorded in stereo, use only the first channel.\n",
    "    \n",
    "    b) Normalize the values of your audio signals to the [−1; 1] range. For this, you need to handle negative and positive values separately and divide by the absolute minimum and maximum possible value, respectively. The documentation lists these extrema values for different data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the energy according to Equation 1 for both files. Use a window size of ̃𝑤 = 10 ms. Note that the unit of the window size 𝑤 in Equation 1 is in samples (and not milliseconds); so, the value 𝑤 you need to use depends on the sampling rate of your recorded audio signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training phase: use the first audio file in the following subtasks.\n",
    "\n",
    "    a) Manually label the regions of silence in your first audio file. You can use any external tool for this task.\n",
    "\n",
    "    b) Store the information of the labelling process in your code e.g. in a boolean array which has the same length as your signal (1 = silence):\n",
    "    \n",
    "    c) Calculate the prior-probabilities 𝑃(𝑆𝑖𝑙𝑒𝑛𝑐𝑒) and 𝑃(𝑉𝑜𝑖𝑐𝑒).\n",
    "    \n",
    "    d) Since we have now continuous features and don’t know the precise distribution from which they arise, we cannot calculate the likelihoods 𝑝(𝑥|𝜔𝑖) directly (as it was the case in the fruit classifier). We will rather assume that the energy values from each class are normally distributed and infer the parameters form the data itself. Build two distributions, one for the energy values from the silence and one for the energy values in the voice regions of your audio file. For this, create scipy.stats.norm objects with the required parameters mean 𝜇 and standard deviation 𝜎 calculated from the corresponding energy values (there are functions in numpy for this job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Testing phase: use the second audio file in the following subtasks.\n",
    "    \n",
    "    a) Classify each energy value from the test file as silence or voice and store the result(e.g. again in a boolean array). To calculate 𝑝(𝑥|𝜔𝑖), you can use the pdf method of the norm object.\n",
    "    \n",
    "    b) Store also the scaled likelihoods used in the classification process, i.e. 𝑝(𝑥|𝑆𝑖𝑙𝑒𝑛𝑐𝑒) ⋅ 𝑃(𝑆𝑖𝑙𝑒𝑛𝑐𝑒) and 𝑝(𝑥|𝑉𝑜𝑖𝑐𝑒) ⋅ 𝑃(𝑉𝑜𝑖𝑐𝑒), in additional arrays so that you can plot them in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Plot the energy values and show the labelled respectively classified classes for both audio files. Additionally, plot the normal distributions from the training phase and the scaled likelihoods values used in the classification process over the signal range."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
